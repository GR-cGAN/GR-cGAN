"""
Our model: GR-cTimeGAN
"""

# Necessary Packages
import logging
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from scipy.ndimage import gaussian_filter1d
from torch.nn.modules.module import Module
from torch.utils.data import DataLoader, RandomSampler
from tqdm import tqdm
from .vae import VanillaVAE

import utils

matplotlib.use('Agg')
matplotlib.rcParams['savefig.dpi'] = 300  # Uncomment for higher plot resolutions
logger = logging.getLogger('GAN.GR-cTimeGAN')


class Embedder(Module):
    """Embedding network between original feature space to latent space.
    Args:
      - X: input time-series features
      - T: input time information
    Returns:
      - H: embeddings
    """

    def __init__(self, input_dim, hidden_dim, num_layers, module_name, dropout, device):
        super(Embedder, self).__init__()
        if module_name == 'gru':
            self.rnn = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)
        else:
            raise ValueError(f'{module_name} is not supported in Embedder.')
        self.linear = nn.Linear(hidden_dim, hidden_dim)
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.device = device

    def forward(self, x):
        hidden_states = torch.zeros(self.num_layers, x.shape[0], self.hidden_dim, device=self.device)
        rnn_output, _ = self.rnn(x, hidden_states)
        h = F.sigmoid(self.linear(rnn_output))
        return h


class Recovery(Module):
    """Recovery network from latent space to original space.
    Args:
      - h: latent representation
      - T: input time information
    Returns:
      - x_tilde: recovered data
    """

    def __init__(self, input_dim, hidden_dim, num_layers, module_name, dropout, device):
        super(Recovery, self).__init__()
        if module_name == 'gru':
            self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)
        else:
            raise ValueError(f'{module_name} is not supported in Embedder.')
        self.linear = nn.Linear(hidden_dim, input_dim)
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.device = device

    def forward(self, h):
        hidden_states = torch.zeros(self.num_layers, h.shape[0], self.hidden_dim, device=self.device)
        rnn_output, _ = self.rnn(h, hidden_states)
        x_tilde = F.sigmoid(self.linear(rnn_output))
        return x_tilde


class Generator(Module):
    """Generator function: Generate time-series data in latent space.
    Args:
      - Z: random variables
      - T: input time information
    Returns:
      - H: generated embedding
    """

    def __init__(self, input_dim, z_dim, hidden_dim, num_layers, module_name, dropout, device):
        super(Generator, self).__init__()
        if module_name == 'gru':
            self.rnn_cond = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)
            self.rnn = nn.GRU(z_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)
        else:
            raise ValueError(f'{module_name} is not supported in Embedder.')
        self.linear = nn.Linear(hidden_dim, hidden_dim)
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.device = device

    def forward(self, cond, z):
        hidden_states = torch.zeros(self.num_layers, z.shape[0], self.hidden_dim, device=self.device)
        cond_output, hidden_states = self.rnn_cond(cond, hidden_states)
        rnn_output, _ = self.rnn(z, hidden_states)
        h = F.sigmoid(self.linear(torch.cat([cond_output, rnn_output], dim=1)))
        return h


class Supervisor(Module):
    """Generate next sequence using the previous sequence.
    Args:
        - H: latent representation
        - T: input time information
    Returns:
        - X_tilde: generated sequence based on the latent representations generated by the generator
    """

    def __init__(self, hidden_dim, num_layers, module_name, dropout, device):
        super(Supervisor, self).__init__()
        if module_name == 'gru':
            self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers - 1, batch_first=True, dropout=dropout)
        else:
            raise ValueError(f'{module_name} is not supported in Embedder.')
        self.linear = nn.Linear(hidden_dim, hidden_dim)
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.device = device

    def forward(self, h):
        hidden_states = torch.zeros(self.num_layers - 1, h.shape[0], self.hidden_dim, device=self.device)
        rnn_output, _ = self.rnn(h, hidden_states)
        x_tilde = F.sigmoid(self.linear(rnn_output))
        return x_tilde


class Discriminator(Module):
    """Discriminate the original and synthetic time-series data.
    Args:
        - h: latent representation
        - T: input time information
    Returns:
        - y_hat: classification results between original and synthetic time-series
    """

    def __init__(self, hidden_dim, num_layers, module_name, dropout, device):
        super(Discriminator, self).__init__()
        if module_name == 'gru':
            self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)
        else:
            raise ValueError(f'{module_name} is not supported in Embedder.')
        self.linear = nn.Linear(hidden_dim, 1)
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.device = device

    def forward(self, h):
        hidden_states = torch.zeros(self.num_layers, h.shape[0], self.hidden_dim, device=self.device)
        rnn_output, _ = self.rnn(h, hidden_states)
        y_logit_hat = self.linear(rnn_output)
        y_hat = F.sigmoid(y_logit_hat)
        return y_logit_hat, y_hat


class Timegan:
    def __init__(self, train_data, ori_data, params):
        """TimeGAN function.
        Use original data as training set to generater synthetic data (time-series)
        Args:
            - ori_data: original time-series data
            - parameters: TimeGAN network parameters
        Returns:
            - generated_data: generated time-series data
        """

        # Basic Parameters
        self.no, self.seq_len, self.input_dim = ori_data.shape
        ori_data = np.float32(ori_data)
        train_data = np.float32(train_data)
        assert self.input_dim == params.input_dim, f'input_dim not match in data_params.'

        self.condition_len = self.seq_len // 3

        # Network Parameters
        hidden_dim = params.model_param.hidden_dim
        num_layers = params.model_param.num_layers
        self.iterations = params.model_param.iterations
        self.batch_size = params.model_param.batch_size
        dropout = params.model_param.dropout
        module_name = params.model_param.module
        self.z_dim = params.z_dim
        self.vae_z_dim = params.model_param.vae_z_dim
        self.vae_hidden_dim = params.model_param.vae_hidden_dim
        self.gamma = params.model_param.gamma
        learning_rate = params.model_param.learning_rate
        self.disc_judge_multiplier = params.disc_judge_multiplier
        self.pred_judge_multiplier = params.pred_judge_multiplier
        self.num_loss = params.model_param.num_loss
        self.device = params.device
        self.lambda_gp = params.model_param.lambda_gp
        self.max_kl_weight = params.model_param.max_kl_weight
        self.params = params

        # Maximum sequence length and each sequence length
        ori_time, self.max_seq_len = utils.extract_time(ori_data)
        restore_path = os.path.join('experiments', self.params.data_name, 'vae_gan', 'base_model',
                                    f'saved_{self.condition_len}_klw_{str(self.max_kl_weight).replace(".", "")}.pth.tar')
        logger.info('Restoring VAE parameters from {}'.format(restore_path))
        self.vae = VanillaVAE(self.input_dim, self.vae_z_dim, self.condition_len, self.vae_hidden_dim,
                              device=self.device, total_itt=self.iterations,
                              max_kl_weight=self.max_kl_weight).to(self.device)
        vae_optim = torch.optim.Adam(self.vae.parameters(), lr=learning_rate)
        self.vae.eval()
        utils.load_checkpoint(restore_path, self.vae, vae_optim)

        # Normalization
        self.ori_data, self.min_val, self.max_val = utils.MinMaxScaler(ori_data)
        self.train_data, _, _ = utils.MinMaxScaler(train_data, self.min_val, self.max_val)

        # Define networks
        self.E = Embedder(self.input_dim, hidden_dim, num_layers, module_name, dropout, self.device).to(self.device)
        self.R = Recovery(self.input_dim, hidden_dim, num_layers, module_name, dropout, self.device).to(self.device)
        self.G = Generator(self.input_dim, self.z_dim, hidden_dim, num_layers, module_name, dropout, self.device).to(
            self.device)
        self.S = Supervisor(hidden_dim, num_layers, module_name, dropout, self.device).to(self.device)
        self.D = Discriminator(hidden_dim, num_layers, module_name, dropout, self.device).to(self.device)
        self.E_optim = torch.optim.Adam(self.E.parameters(), lr=learning_rate)
        self.R_optim = torch.optim.Adam(self.R.parameters(), lr=learning_rate)
        self.G_optim = torch.optim.Adam(self.G.parameters(), lr=learning_rate)
        self.S_optim = torch.optim.Adam(self.S.parameters(), lr=learning_rate)
        self.D_optim = torch.optim.Adam(self.D.parameters(), lr=learning_rate)
        self.E_loss_T0 = nn.MSELoss()
        self.G_loss_S = nn.MSELoss()
        self.G_loss_U = nn.BCEWithLogitsLoss()
        self.G_loss_U_e = nn.BCEWithLogitsLoss()
        self.G_loss_V1 = nn.L1Loss()
        self.G_loss_V2 = nn.L1Loss()
        self.D_loss_real = nn.BCEWithLogitsLoss()
        self.D_loss_fake = nn.BCEWithLogitsLoss()
        self.D_loss_fake_e = nn.BCEWithLogitsLoss()

        self.gp_loss = nn.MSELoss()

    def fit(self, restore_iteration=-2):
        # to visualize loss in the third stage
        loss_summary = np.zeros((self.iterations, self.num_loss))

        start_iteration = restore_iteration + 1
        dataset = utils.TS_sampler(self.train_data)
        sampler = RandomSampler(dataset)
        dataloader = DataLoader(dataset, self.batch_size, sampler=sampler, num_workers=8)
        it = iter(dataloader)
        if start_iteration == -1:
            # 1. Embedding network training
            logger.info('Start Embedding Network Training')
            for itt in tqdm(range(self.iterations)):
                self.E_optim.zero_grad()
                self.R_optim.zero_grad()

                # Set mini-batch
                try:  # https://stackoverflow.com/a/58876890/8365622
                    # Samples the batch
                    x_mb = next(it).to(self.device)
                except StopIteration:
                    # restart the generator if the previous generator is exhausted.
                    it = iter(dataloader)
                    x_mb = next(it).to(self.device)
                # Train embedder & recovery
                h_mb = self.E(x_mb)
                x_tilde_mb = self.R(h_mb)
                step_e_loss = self.E_loss_T0(x_mb, x_tilde_mb)
                step_e_loss.backward()
                self.E_optim.step()
                self.R_optim.step()

                # Checkpoint
                if (itt + 1) % 1000 == 0:
                    logger.info(f'step: {itt}/{self.iterations}, e_loss: {np.round(np.sqrt(step_e_loss.item()), 4)}')

            logger.info('Finish Embedding Network Training')

            # 2. Training only with supervised loss
            logger.info('Start Training with Supervised Loss Only')
            for itt in tqdm(range(self.iterations)):
                self.S_optim.zero_grad()

                # Set mini-batch
                try:  # https://stackoverflow.com/a/58876890/8365622
                    # Samples the batch
                    x_mb = next(it).to(self.device)
                except StopIteration:
                    # restart the generator if the previous generator is exhausted.
                    it = iter(dataloader)
                    x_mb = next(it).to(self.device)
                # Train generator
                h_mb = self.E(x_mb)
                h_hat_supervise_mb = self.S(h_mb)
                step_g_loss_s = self.G_loss_S(h_mb[:, 1:, :], h_hat_supervise_mb[:, :-1, :])
                step_g_loss_s.backward()
                self.G_optim.step()
                self.S_optim.step()

                # Checkpoint
                if (itt + 1) % 1000 == 0:
                    logger.info(f'step: {itt}/{self.iterations}, s_loss: {np.sqrt(step_g_loss_s.item()):.4f}')

            logger.info('Finish Training with Supervised Loss Only')

            utils.save_checkpoint({'epoch': -1,
                                   'state_dict': self.E.state_dict(),
                                   'optim_dict': self.E_optim.state_dict()},
                                  itt=-1,
                                  checkpoint=self.params.model_dir,
                                  ins_name='E')
            utils.save_checkpoint({'epoch': -1,
                                   'state_dict': self.R.state_dict(),
                                   'optim_dict': self.R_optim.state_dict()},
                                  itt=-1,
                                  checkpoint=self.params.model_dir,
                                  ins_name='R')
            utils.save_checkpoint({'epoch': -1,
                                   'state_dict': self.G.state_dict(),
                                   'optim_dict': self.G_optim.state_dict()},
                                  itt=-1,
                                  checkpoint=self.params.model_dir,
                                  ins_name='G')
            utils.save_checkpoint({'epoch': -1,
                                   'state_dict': self.S.state_dict(),
                                   'optim_dict': self.S_optim.state_dict()},
                                  itt=-1,
                                  checkpoint=self.params.model_dir,
                                  ins_name='S')
            utils.save_checkpoint({'epoch': -1,
                                   'state_dict': self.D.state_dict(),
                                   'optim_dict': self.D_optim.state_dict()},
                                  itt=-1,
                                  checkpoint=self.params.model_dir,
                                  ins_name='D')

        else:
            if restore_iteration == -1:
                restore_dir = self.params.base_model_dir
            else:
                restore_dir = self.params.model_dir
            restore_path = os.path.join(restore_dir, f'itt_{restore_iteration}_ins_E.pth.tar')
            logger.info('Restoring parameters from {}'.format(restore_path))
            utils.load_checkpoint(restore_path, self.E, self.E_optim)
            restore_path = os.path.join(restore_dir, f'itt_{restore_iteration}_ins_R.pth.tar')
            logger.info('Restoring parameters from {}'.format(restore_path))
            utils.load_checkpoint(restore_path, self.R, self.R_optim)
            restore_path = os.path.join(restore_dir, f'itt_{restore_iteration}_ins_G.pth.tar')
            logger.info('Restoring parameters from {}'.format(restore_path))
            utils.load_checkpoint(restore_path, self.G, self.G_optim)
            restore_path = os.path.join(restore_dir, f'itt_{restore_iteration}_ins_S.pth.tar')
            logger.info('Restoring parameters from {}'.format(restore_path))
            utils.load_checkpoint(restore_path, self.S, self.S_optim)
            restore_path = os.path.join(restore_dir, f'itt_{restore_iteration}_ins_D.pth.tar')
            logger.info('Restoring parameters from {}'.format(restore_path))
            utils.load_checkpoint(restore_path, self.D, self.D_optim)
            restore_path = os.path.join(restore_dir, f'itt_{restore_iteration}_loss.npy')
            if restore_iteration > -1:
                loss_summary[: restore_iteration + 1] = np.load(restore_path)

        # 3. Joint Training
        logger.info('Start Joint Training')
        loss_cache = np.zeros((self.params.model_param.save_iteration, self.num_loss))

        start_iteration = max(0, start_iteration)
        for itt in tqdm(range(start_iteration, self.iterations)):
            itt_cache = itt % self.params.model_param.save_iteration

            # Generator training (twice more than discriminator training)
            for kk in range(2):
                self.G_optim.zero_grad()
                self.S_optim.zero_grad()

                # Set mini-batch
                try:  # https://stackoverflow.com/a/58876890/8365622
                    # Samples the batch
                    x_mb = next(it).to(self.device)
                except StopIteration:
                    # restart the generator if the previous generator is exhausted.
                    it = iter(dataloader)
                    x_mb = next(it).to(self.device)
                h_mb = self.E(x_mb)

                # Random vector generation
                z_mb = torch.rand((x_mb.shape[0], self.max_seq_len - self.condition_len, self.z_dim),
                                  device=self.device)
                e_hat_mb = torch.cat([self.E(x_mb[:, :self.condition_len]),
                                      self.G(x_mb[:, :self.condition_len], z_mb)[:, self.condition_len:]], dim=1)
                h_hat_mb = self.S(e_hat_mb)
                h_hat_supervise_mb = self.S(h_mb)

                # Synthetic data
                x_hat_mb = self.R(h_hat_mb)

                # Discriminator
                Y_logits_fake, Y_fake = self.D(h_hat_mb)
                Y_logits_fake_e, Y_fake_e = self.D(e_hat_mb)

                # Train generator
                # 1. Adversarial loss
                step_g_loss_u = self.G_loss_U(input=Y_logits_fake, target=torch.ones_like(Y_fake))
                step_g_loss_u_e = self.G_loss_U_e(input=Y_logits_fake_e, target=torch.ones_like(Y_fake_e))

                # 2. Supervised loss
                step_g_loss_s = self.G_loss_S(h_mb[:, 1:, :], h_hat_supervise_mb[:, :-1, :])

                # 3. Two Moments
                step_g_loss_v1 = self.G_loss_V1(x_hat_mb.std(dim=0), x_mb.std(dim=0))
                step_g_loss_v2 = self.G_loss_V2(x_hat_mb.mean(dim=0), x_mb.mean(dim=0))
                step_g_loss_v = step_g_loss_v1 + step_g_loss_v2

                # sample two batch of x, encode with vae, sample two points from interpolate mu, sqrt(sigma1^2 + sigma2^2)
                # get decoded conditions. Encode conditions with E, feed into G. Soft DTW.
                g_gp = self.calc_gp(x_mb[:, :self.condition_len],
                                    torch.rand((x_mb.shape[0], self.max_seq_len - self.condition_len, self.z_dim),
                                               device=self.device))

                # 4. Summation
                step_g_loss = step_g_loss_u + self.gamma * step_g_loss_u_e + \
                              100 * torch.sqrt(step_g_loss_s) + 100 * step_g_loss_v + self.lambda_gp * g_gp
                loss_cache[itt_cache, 0] = step_g_loss_u.item()
                loss_cache[itt_cache, 1] = self.gamma * step_g_loss_u_e.item()
                loss_cache[itt_cache, 2] = 100 * torch.sqrt(step_g_loss_s).item()
                loss_cache[itt_cache, 3] = 100 * step_g_loss_v1.item()
                loss_cache[itt_cache, 4] = 100 * step_g_loss_v2.item()
                loss_cache[itt_cache, 5] = self.lambda_gp * g_gp.item()
                step_g_loss.backward()
                self.G_optim.step()
                self.S_optim.step()

                self.E_optim.zero_grad()
                self.R_optim.zero_grad()
                # Set mini-batch
                try:  # https://stackoverflow.com/a/58876890/8365622
                    # Samples the batch
                    x_mb = next(it).to(self.device)
                except StopIteration:
                    # restart the generator if the previous generator is exhausted.
                    it = iter(dataloader)
                    x_mb = next(it).to(self.device)

                h_mb = self.E(x_mb)
                x_tilde_mb = self.R(h_mb)
                h_hat_supervise_mb = self.S(h_mb)
                step_g_loss_s = self.G_loss_S(h_mb[:, 1:, :], h_hat_supervise_mb[:, :-1, :])
                step_e_loss0 = 10 * torch.sqrt(self.E_loss_T0(x_mb, x_tilde_mb))
                step_e_loss = step_e_loss0 + 0.1 * step_g_loss_s
                loss_cache[itt_cache, 6] = step_e_loss0.item()
                loss_cache[itt_cache, 7] = 0.1 * step_g_loss_s.item()
                step_e_loss.backward()
                self.E_optim.step()
                self.R_optim.step()

            # Discriminator training
            # Set mini-batch
            try:  # https://stackoverflow.com/a/58876890/8365622
                # Samples the batch
                x_mb = next(it).to(self.device)
            except StopIteration:
                # restart the generator if the previous generator is exhausted.
                it = iter(dataloader)
                x_mb = next(it).to(self.device)
            h_mb = self.E(x_mb)

            # Random vector generation
            z_mb = torch.rand((x_mb.shape[0], self.max_seq_len - self.condition_len, self.z_dim), device=self.device)
            e_hat_mb = torch.cat([self.E(x_mb[:, :self.condition_len]),
                                  self.G(x_mb[:, :self.condition_len], z_mb)[:, self.condition_len:]], dim=1)
            h_hat_mb = self.S(e_hat_mb)

            # Discriminator
            Y_logits_fake, Y_fake = self.D(h_hat_mb)
            Y_logits_real, Y_real = self.D(h_mb)
            Y_logits_fake_e, Y_fake_e = self.D(e_hat_mb)

            self.D_optim.zero_grad()
            step_d_loss_real = self.D_loss_real(input=Y_logits_real, target=torch.ones_like(Y_real))
            step_d_loss_fake = self.D_loss_fake(input=Y_logits_fake, target=torch.zeros_like(Y_fake))
            step_d_loss_fake_e = self.D_loss_fake_e(input=Y_logits_fake_e, target=torch.zeros_like(Y_fake_e))
            step_d_loss = step_d_loss_real + step_d_loss_fake + self.gamma * step_d_loss_fake_e

            if step_d_loss > 0.15:
                loss_cache[itt_cache, 8] = step_d_loss_real.item()
                loss_cache[itt_cache, 9] = step_d_loss_fake.item()
                loss_cache[itt_cache, 10] = step_d_loss_fake_e.item()
                step_d_loss.backward()
                self.D_optim.step()
            else:
                loss_cache[itt_cache, 8:] = 0

            # Print multiple checkpoints and visualize generated samples
            if itt % 100 == 0:
                logger.info(f'step: {itt}/{self.iterations}, g_loss_u: {step_g_loss_u.item():.4f}, '
                            f'g_loss_u_e: {self.gamma * step_g_loss_u_e.item():.4f}, g_loss_v: {step_g_loss_v.item():.4f}\n'
                            f'gradient_penalty: {self.lambda_gp * g_gp.item():.4f}\n'
                            f'g_loss_s: {100 * np.sqrt(step_g_loss_s.item()):.4f}, '
                            f'e_loss_t0: {step_e_loss0.item() / 10:.4f}, step_d_loss: {step_d_loss.item():.4f}')

            if itt % self.params.model_param.save_iteration == 0:
                if itt != 0:
                    loss_summary[itt + 1 - self.params.model_param.save_iteration: itt + 1] = loss_cache.copy()
                    np.save(os.path.join(self.params.model_dir, f'itt_{itt}_loss'), loss_summary[: itt + 1])
                    plot_all_loss(loss_summary[: itt + 1], itt, self.params, location=self.params.plot_dir)

                    utils.save_checkpoint({'epoch': itt,
                                           'state_dict': self.E.state_dict(),
                                           'optim_dict': self.E_optim.state_dict()},
                                          itt=itt,
                                          checkpoint=self.params.model_dir,
                                          ins_name='E')
                    utils.save_checkpoint({'epoch': itt,
                                           'state_dict': self.R.state_dict(),
                                           'optim_dict': self.R_optim.state_dict()},
                                          itt=itt,
                                          checkpoint=self.params.model_dir,
                                          ins_name='R')
                    utils.save_checkpoint({'epoch': itt,
                                           'state_dict': self.G.state_dict(),
                                           'optim_dict': self.G_optim.state_dict()},
                                          itt=itt,
                                          checkpoint=self.params.model_dir,
                                          ins_name='G')
                    utils.save_checkpoint({'epoch': itt,
                                           'state_dict': self.S.state_dict(),
                                           'optim_dict': self.S_optim.state_dict()},
                                          itt=itt,
                                          checkpoint=self.params.model_dir,
                                          ins_name='S')
                    utils.save_checkpoint({'epoch': itt,
                                           'state_dict': self.D.state_dict(),
                                           'optim_dict': self.D_optim.state_dict()},
                                          itt=itt,
                                          checkpoint=self.params.model_dir,
                                          ins_name='D')

                with torch.no_grad():
                    self.E.eval()
                    self.R.eval()
                    self.G.eval()
                    self.S.eval()
                    self.D.eval()
                    if self.ori_data.shape[0] == self.train_data.shape[0]:
                        sampled_x = self.ori_data[np.random.choice(self.ori_data.shape[0], size=4, replace=False)]
                    else:
                        eval_sz = self.ori_data.shape[0] - self.train_data.shape[0]
                        sampled_x = self.ori_data[np.random.choice(eval_sz,
                                                                   size=4, replace=False) + self.train_data.shape[0]]
                    x_mb = torch.from_numpy(np.float32(sampled_x)).to(self.device)
                    h_mb = self.E(x_mb)
                    x_tilde_mb = self.R(h_mb)
                    h_hat_supervise_mb = self.S(h_mb)
                    x_hat_mb = self.R(h_hat_supervise_mb)

                    z_mb = torch.rand((12, self.max_seq_len - self.condition_len, self.z_dim), device=self.device)
                    if self.ori_data.shape[0] == self.train_data.shape[0]:
                        selected_index = np.random.choice(self.train_data.shape[0], size=12, replace=False)
                        sampled_cond = self.ori_data[selected_index, :, :]
                    else:
                        seen_index = np.random.choice(self.train_data.shape[0], size=6, replace=False)
                        unseen_index = np.random.choice(eval_sz, size=6, replace=False) + self.train_data.shape[0]
                        sampled_cond = np.concatenate([self.ori_data[seen_index, :, :],
                                                       self.ori_data[unseen_index, :, :]], axis=0)
                    cond_mb = torch.from_numpy(np.float32(sampled_cond)).to(self.device)
                    e_hat_mb = torch.cat([self.E(cond_mb[:, :self.condition_len]),
                                          self.G(cond_mb[:, :self.condition_len], z_mb)[:, self.condition_len:]], dim=1)
                    h_hat_mb = self.S(e_hat_mb)
                    generated_data = self.R(h_hat_mb).data.cpu().numpy()
                    self.E.train()
                    self.R.train()
                    self.G.train()
                    self.S.train()
                    self.D.train()

                utils.plot_condition_samples(self.params.plot_dir, itt, sampled_x, x_tilde_mb.data.cpu().numpy(),
                                             x_hat_mb.data.cpu().numpy(), generated_data, sampled_cond, self.params,
                                             self.condition_len)

        logger.info('Finish Joint Training')

    def generate_data_given_size(self, multiplier):
        # Synthetic data generation
        self.E.eval()
        self.R.eval()
        self.G.eval()
        self.S.eval()
        self.D.eval()
        with torch.no_grad():
            gen_size = self.no * multiplier
            generated_data = torch.zeros(gen_size, self.seq_len, self.input_dim, device=self.device)
            original_data = torch.from_numpy(np.float32(np.tile(self.ori_data, (multiplier, 1, 1)))).to(
                self.device)
            predict_batch_size = 512
            for i in range(predict_batch_size, gen_size, predict_batch_size):
                z_mb = torch.rand((predict_batch_size, self.max_seq_len - self.condition_len, self.z_dim),
                                  device=self.device)
                ori_cond_mb = original_data[i - predict_batch_size: i, :self.condition_len]
                e_hat_mb = torch.cat([self.E(ori_cond_mb),
                                      self.G(ori_cond_mb, z_mb)[:, self.condition_len:]], dim=1)
                h_hat_mb = self.S(e_hat_mb)
                generated_data[i - predict_batch_size: i] = self.R(h_hat_mb)

            left_batch_size = gen_size % predict_batch_size
            if left_batch_size > 0:
                z_mb = torch.randn((left_batch_size, self.max_seq_len - self.condition_len, self.z_dim),
                                   device=self.device)
                ori_cond_mb = original_data[-left_batch_size:, :self.condition_len]
                e_hat_mb = torch.cat([self.E(ori_cond_mb),
                                      self.G(ori_cond_mb, z_mb)[:, self.condition_len:]], dim=1)
                h_hat_mb = self.S(e_hat_mb)
                generated_data[-left_batch_size:] = self.R(h_hat_mb)

            # Renormalization
            generated_data = generated_data.data.cpu().numpy() * self.max_val
            generated_data = generated_data + self.min_val
        return generated_data

    def calc_gp(self, batch_conds, z):
        b_sz = batch_conds.shape[0]
        alpha = torch.rand(b_sz, 1, device=self.device)
        mu1, log_var1 = self.vae.encode(batch_conds)
        mu2, log_var2 = self.vae.encode(batch_conds[torch.randperm(b_sz)])
        interpolates_mu = alpha * mu1 + (1 - alpha) * mu2
        # interpolates_log_var = torch.sqrt(alpha * torch.exp(log_var1) + (1 - alpha) * torch.exp(log_var2))
        # noise_std = torch.rand_like(log_var1) * 0.1 * torch.exp(0.5 * interpolates_log_var)
        noise_std = 0.1
        interpolate_cond = self.vae.decode(interpolates_mu)
        noise_cond = self.vae.decode(interpolates_mu + noise_std)
        batch_interpolates = self.G(interpolate_cond.detach(), z.detach())
        batch_noise = self.G(noise_cond.detach(), z.detach())
        # gp = torch.mean(sdtw(batch_interpolates, batch_noise))
        gp = self.gp_loss(batch_interpolates[:, self.condition_len:], batch_noise[:, self.condition_len:])
        return gp


def cum_by_axis1(input_x):
    cum_input = np.zeros(input_x.shape)
    for i in range(cum_input.shape[1]):
        cum_input[:, i] = np.sum(input_x[:, :(i + 1)], axis=1)
    return cum_input


def plot_all_loss(loss_summary, plot_num, params, location='./figures/'):
    gaussian_window_size = 1
    num_itt = loss_summary.shape[0]
    color_list = ['b', 'g', 'r', 'c', 'darkred', 'silver', 'm', 'y', 'b', 'pink']
    x = np.arange(num_itt)
    f = plt.figure(figsize=(10, 5), constrained_layout=True)
    ax = f.subplots(3)
    f.suptitle(f'{params.plot_title}/it_{plot_num}')

    # GS loss
    num_GS_loss = 6
    GS_cum = cum_by_axis1(loss_summary[:, :num_GS_loss])
    GS_cum = gaussian_filter1d(GS_cum, gaussian_window_size, axis=0)
    loss_list = ['step_g_loss_u', 'step_g_loss_u_e', 'step_g_loss_s', 'step_g_loss_v1', 'step_g_loss_v2', 'gp']
    ax[0].fill_between(x, 0, GS_cum[:, 0], color=color_list[0], alpha=0.2, label=loss_list[0])
    for i in range(1, num_GS_loss):
        ax[0].fill_between(x, GS_cum[:, i - 1], GS_cum[:, i], color=color_list[i], alpha=0.2, label=loss_list[i])
    ax[0].legend(loc='upper left', bbox_to_anchor=(1.05, 1), fancybox=True)

    # ER loss
    num_ER_loss = 2
    ER_cum = cum_by_axis1(loss_summary[:, num_GS_loss: num_GS_loss + num_ER_loss])
    ER_cum = gaussian_filter1d(ER_cum, gaussian_window_size, axis=0)
    loss_list = ['step_e_loss0', 'step_g_loss_s']
    ax[1].fill_between(x, 0, ER_cum[:, 0], color=color_list[0], alpha=0.2, label=loss_list[0])
    for i in range(1, num_ER_loss):
        ax[1].fill_between(x, ER_cum[:, i - 1], ER_cum[:, i], color=color_list[i], alpha=0.2, label=loss_list[i])
    ax[1].legend(loc='upper left', bbox_to_anchor=(1.05, 1), fancybox=True)

    # D loss
    num_D_loss = 3
    D_cum = cum_by_axis1(loss_summary[:, num_GS_loss + num_ER_loss:])
    D_cum = gaussian_filter1d(D_cum, gaussian_window_size, axis=0)
    loss_list = ['step_d_loss_real', 'step_d_loss_fake', 'step_d_loss_fake_e']
    ax[2].fill_between(x, 0, D_cum[:, 0], color=color_list[0], alpha=0.2, label=loss_list[0])
    for i in range(1, num_D_loss):
        ax[2].fill_between(x, D_cum[:, i - 1], D_cum[:, i], color=color_list[i], alpha=0.2, label=loss_list[i])
    ax[2].legend(loc='upper left', bbox_to_anchor=(1.05, 1), fancybox=True)

    f.savefig(os.path.join(location, f'{plot_num}_summary.png'))
    plt.close()
